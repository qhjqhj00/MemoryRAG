{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45865d91-833e-4cab-bb14-b20d3aa003ae",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-06T08:13:32.384995Z",
     "iopub.status.busy": "2024-08-06T08:13:32.384744Z",
     "iopub.status.idle": "2024-08-06T08:13:36.081879Z",
     "shell.execute_reply": "2024-08-06T08:13:36.081251Z",
     "shell.execute_reply.started": "2024-08-06T08:13:32.384971Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "08/06/2024 16:13:35 - INFO - faiss.loader - Loading faiss with AVX512 support.\n",
      "08/06/2024 16:13:35 - INFO - faiss.loader - Successfully loaded faiss with AVX512 support.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "from semantic_text_splitter import TextSplitter\n",
    "import tiktoken\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "from src.retriever import DenseRetriever\n",
    "from tasks.longbench_utils import qa_f1_score\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text_splitter = TextSplitter.from_tiktoken_model(\"gpt-3.5-turbo\", 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1471bf-a8c2-48bb-9a70-52e2fa7a8813",
   "metadata": {},
   "source": [
    "### initialize hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c864be-8296-464e-8d60-af71494812ab",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-06T08:13:36.089991Z",
     "iopub.status.busy": "2024-08-06T08:13:36.089517Z",
     "iopub.status.idle": "2024-08-06T08:13:36.094129Z",
     "shell.execute_reply": "2024-08-06T08:13:36.093625Z",
     "shell.execute_reply.started": "2024-08-06T08:13:36.089973Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "        \"device_map\": \"cuda:0\",\n",
    "        \"attn_implementation\": \"sdpa\",\n",
    "        \"torch_dtype\": \ttorch.bfloat16,\n",
    "        \"trust_remote_code\": True,\n",
    "        \"ultragist_ratio\": [4]\n",
    "    }\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"padding_side\": \"left\",\n",
    "        \"trust_remote_code\": True,\n",
    "    }\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0\n",
    "}\n",
    "\n",
    "prompts = {\n",
    "  \"summarization\": \"You are provided with a long article. Your task is to generate a concise summary by listing the key points of the long article.\\n\\n\\\n",
    "  ### Instructions:\\n\\n1. Long Article: {context}\\n2. Output: Generate a list of key points, each separated by a newline, with numeric order.\\n\\n\\\n",
    "  ### Requirements:\\n\\n- The key points should be short and high-level.\\n- Ensure that the key points convey the most important information and main events of the long article.\\n\",\n",
    "  \"clues_1\": \"You are given a long article and a question. After a quick read-through, you have a rough memory of the article. To answer the question effectively, \\\n",
    "  you need to recall and extract specific details from the article. Your task is to find and retrieve the relevant clue texts from the article that will help answer the question.\\\n",
    "  \\n\\n### Inputs:\\n- **Long Article:** {context}\\n- **Question:** {input}\\n\\n### Requirements:\\n1. You have a general understanding of the article. Your task is to identify and \\\n",
    "  extract one or more specific clue texts from the article that are relevant to the question.\\n2. Output only the extracted clue texts. For multiple sentences, separate them with a newline.\\n\",\n",
    "  \"clues_2\": \"You are provided with a long article and a question. After a quick read-through, you have a rough memory of the article. To better answer the question,\\\n",
    "  you need to recall specific details within the article. Your task is to generate precise clue questions that can help locate the necessary information.\\n\\n### \\\n",
    "  Inputs:\\n- **Long Article:** {context}\\n- **Question:** {input}\\n\\n### Requirements:\\n1. You have a general understanding of the article. Your task is to write \\\n",
    "  one or more precise clue questions to search for supporting evidence in the article.\\n2. Output only the clue questions. For multiple questions, separate them with a newline.\\n \",\n",
    "  \"qa\": \"You are given a long text. You're required to read the long text and answer the questions.\\n\\nNow the long text begins. \\n\\n{context}\\n\\nNow the \\\n",
    "  long text ends.\\n\\nAnswer the following questions.\\n\\n{input}\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663936a2-01c6-490d-83ba-abfebbf1176e",
   "metadata": {},
   "source": [
    "### initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b21334-5614-4a8f-93a4-616200513914",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"path_to_checkpoint\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            **tokenizer_kwargs)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, \n",
    "            **model_kwargs,\n",
    "        ).eval()\n",
    "\n",
    "retriever = DenseRetriever(\n",
    "            encoder=\"BAAI/bge-m3\",\n",
    "            pooling_method=\"cls\",\n",
    "            dense_metric=\"cos\",\n",
    "            query_max_length=128,\n",
    "            key_max_length=512,\n",
    "            cache_dir=\"/share/shared_models\",\n",
    "            hits=3,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "            **tokenizer_kwargs)\n",
    "\n",
    "model_kwargs.pop('ultragist_ratio')\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "            **model_kwargs,\n",
    "        ).eval()\n",
    "generator_tokenizer.pad_token = generator_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e23015-373c-4575-a0d2-2be4d4b4a183",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ccd937-32b1-4529-b106-7dab50619648",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-06T08:17:51.857406Z",
     "iopub.status.busy": "2024-08-06T08:17:51.856845Z",
     "iopub.status.idle": "2024-08-06T08:17:51.867367Z",
     "shell.execute_reply": "2024-08-06T08:17:51.866809Z",
     "shell.execute_reply.started": "2024-08-06T08:17:51.857375Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompts):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "    to_encode = []\n",
    "    for p in prompts:\n",
    "        to_encode.append(tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": p}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True))\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        to_encode, add_special_tokens=False, return_tensors=\"pt\", padding=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "                **inputs,\n",
    "                **generation_kwargs,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "    outputs = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    model.memory.reset()\n",
    "    return outputs\n",
    "\n",
    "def load_jsonl(path):\n",
    "    rtn = []\n",
    "    for line in tqdm(open(path)):\n",
    "        rtn.append(json.loads(line))\n",
    "    return rtn\n",
    "\n",
    "def show_sample(line):\n",
    "    print(\"Context Length: \", line[\"length\"])\n",
    "    print(\"Input Query: \", line[\"input\"])\n",
    "    print(\"Answer: \", line[\"answers\"][0])\n",
    "    print(\"===\"*20)\n",
    "    \n",
    "def get_clues(sample):\n",
    "    rtn = []\n",
    "    output = []\n",
    "    output.append(generate(\n",
    "         prompts[\"clues_1\"].format(context=sample[\"context\"], input=sample[\"input\"]))[0])\n",
    "    output.append(generate(\n",
    "         prompts[\"clues_2\"].format(context=sample[\"context\"], input=sample[\"input\"]))[0])\n",
    "    \n",
    "    for line in output:\n",
    "        rtn.extend(line.split(\"\\n\"))\n",
    "    rtn = [sent for sent in rtn if len(sent.split()) > 3]\n",
    "    rtn = list(set(rtn))\n",
    "        \n",
    "    return rtn\n",
    "\n",
    "def pred(prompts):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "    to_encode = []\n",
    "    for p in prompts:\n",
    "        to_encode.append(tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": p}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True))\n",
    "        \n",
    "    inputs = generator_tokenizer(\n",
    "        to_encode, add_special_tokens=False, return_tensors=\"pt\", padding=True\n",
    "    ).to(generator_model.device)\n",
    "    \n",
    "    outputs = generator_model.generate(\n",
    "                **inputs,\n",
    "                **generation_kwargs,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "    outputs = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    outputs = generator_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs\n",
    "\n",
    "def get_evidence(queries, context):\n",
    "    retriever.remove_all()\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    retrieval_corpus = text_splitter.chunks(context)\n",
    "    print(f\"{len(retrieval_corpus)} passages will be indexed...\")\n",
    "    \n",
    "    retriever.add(retrieval_corpus)\n",
    "    topk_scores, topk_indices = retriever.search(\n",
    "            queries=queries)\n",
    "    \n",
    "    # reorder indices to keep consecutive chunks\n",
    "    topk_indices = list(chain(*[topk_index.tolist() for topk_index in topk_indices]))\n",
    "    topk_indices = list(set(topk_indices))\n",
    "\n",
    "    topk_indices = sorted([x for x in topk_indices if x > -1])\n",
    "    print(\"Mem Selected indices: \", topk_indices)\n",
    "    # slice out relevant context from the corpus\n",
    "    mem_results = [retrieval_corpus[i].strip() for i in topk_indices]\n",
    "    \n",
    "    topk_scores, topk_indices = retriever.search(\n",
    "            queries=queries[-1:], hits=len(mem_results))\n",
    "    \n",
    "    topk_indices = topk_indices[0].tolist()\n",
    "    topk_indices = sorted([x for x in topk_indices if x > -1])\n",
    "    print(\"RAG Selected indices: \", topk_indices)\n",
    "    rag_results = [retrieval_corpus[i].strip() for i in topk_indices]\n",
    "    \n",
    "    return mem_results, rag_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52be4a6-a6d1-4042-b556-d5cff94f5c95",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f66a73-efa9-4f93-82df-13be5d9c47e8",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-06T08:14:59.095664Z",
     "iopub.status.busy": "2024-08-06T08:14:59.094952Z",
     "iopub.status.idle": "2024-08-06T08:15:00.097000Z",
     "shell.execute_reply": "2024-08-06T08:15:00.096461Z",
     "shell.execute_reply.started": "2024-08-06T08:14:59.095640Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "438it [00:00, 439.71it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_data = load_jsonl(\n",
    "    \"legal.dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209ec6bd-33f3-4f4d-b2e1-d9f2478850f7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-06T08:25:19.717538Z",
     "iopub.status.busy": "2024-08-06T08:25:19.716996Z",
     "iopub.status.idle": "2024-08-06T08:26:24.639698Z",
     "shell.execute_reply": "2024-08-06T08:26:24.639124Z",
     "shell.execute_reply.started": "2024-08-06T08:25:19.717502Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Length:  56441\n",
      "Input Query:  What are the representations and warranties of the Company regarding its qualification, organization, and subsidiaries?\n",
      "Answer:  The Company represents and warrants that it and its subsidiaries are duly organized, validly existing, and in good standing under the laws of their respective jurisdictions, have all requisite power and authority to own their properties and assets, and are duly qualified to do business and in good standing in each jurisdiction where required.\n",
      "============================================================\n",
      "Clues from Memory:\n",
      " The Company and each of its Subsidiaries is a legal entity duly organized, validly existing and in good standing (or the equivalent thereof, if applicable, in each case, with respect to the jurisdictions that recognize the concept of good standing or any equivalent thereof) under the Laws of the jurisdiction of its incorporation, organization or formation, as applicable, and has all requisite corporate or similar power and authority to own, lease and operate its properties and assets, and to carry on the Business as present\n",
      "What are the specific representations and warranties of the Company regarding its qualification, organization, and subsidiaries as stated in Section 5.1 of the article?\n",
      "Section 5.1 Qualification, Organization, Subsidiaries, etc.\n",
      "============================================================\n",
      "161 passages will be indexed...\n",
      "Mem Selected indices:  [48, 49, 90, 91, 92, 100, 112, 142]\n",
      "RAG Selected indices:  [48, 62, 86, 90, 91, 96, 110, 112]\n",
      "Context Compression Rate for MemRAG:  3.9 %\n",
      "Context Compression Rate for RAG:  4.8 %\n",
      "MemRAG score:  0.603\n",
      "RAG score:  0.441\n"
     ]
    }
   ],
   "source": [
    "sample_id = random.randint(0,len(dev_data)-1)\n",
    "sample = dev_data[sample_id]\n",
    "\n",
    "show_sample(sample)\n",
    "clues = get_clues(sample)\n",
    "print(\"Clues from Memory:\\n\", \"\\n\".join(clues))\n",
    "print(\"===\"*20)\n",
    "\n",
    "clues.append(sample[\"input\"])\n",
    "mem_evidence, rag_evidence = get_evidence(clues, sample[\"context\"])\n",
    "mem_results = pred(prompts[\"qa\"].format(context=\"\\n\\n\".join(mem_evidence), input=sample[\"input\"]))\n",
    "rag_results = pred(prompts[\"qa\"].format(context=\"\\n\\n\".join(rag_evidence), input=sample[\"input\"]))\n",
    "\n",
    "print(\"Context Compression Rate for MemRAG: \", round(len(encoder.encode(\"\\n\\n\".join(mem_evidence)))/sample[\"length\"], 3)*100, \"%\")\n",
    "print(\"Context Compression Rate for RAG: \", round(len(encoder.encode(\"\\n\\n\".join(rag_evidence)))/sample[\"length\"], 3)*100, \"%\")\n",
    "\n",
    "print(\"MemRAG score: \", round(qa_f1_score(mem_results[0], sample[\"answers\"][0]),3))\n",
    "print(\"RAG score: \", round(qa_f1_score(rag_results[0], sample[\"answers\"][0]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9419a9-ab8f-4c35-a15b-5559d29d9883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
